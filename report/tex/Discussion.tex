\section{Discussion and Future Work}
\label{sec:discussion}

\subsection{Limitations}
Firstly, the sonar reading is determined useful only when almost all the grid cells within the sonar beam are marked as free space in the depth-camera-based occupancy map. However, due to the inherent inaccuracies and wide-angle nature of the actual sonar sensor (0.5 rad), this condition can only be satisfied when the drone is positioned very close to the window region and is facing directly at the main part of the window, rather than the window edge. Thus, detecting window regions using sonar requires the drone to be in very close proximity and to have a clear view of the window, limiting its usefulness in scenarios where the drone cannot get close enough or has obstructed views.

Secondly, the RANSAC-based plane segmentation technique may not be robust enough to handle real-life indoor settings where there are many obstacles such as tables, chairs, and other objects that can interfere with the segmentation process. This could result in incorrect segmentation and affect the accuracy of the final window detection.

Furthermore, the window segmentation approach assumes that the window is surrounded by solid, camera-visible walls, which may not always be the case in real-life scenarios. For example, some large windows may only have thin metal frames among the window patches, which could affect the segmentation process. Additionally, the approach may not be applicable to large tilted windows such as the one found in the PERCH meeting room. Therefore, while the proposed approach provides a good starting point, further research and development is needed to address these limitations and ensure its effectiveness in a wider range of scenarios.

\subsection{Future Work}
The current system faces certain bottleneck due to the limitations of the sonar hardware used, which provides only a scalar reading in the frequency around 8Hz. When compared to a typical depth camera that can provide a depth image of 720x480 or larger, the single sonar measurement is significantly limited and of low accuracy. In some prior work, researchers have utilized sonar arrays, with each sensor pointing at different angles. However, this approach is mostly limited to ground vehicles as it adds significant payload to a \gls{uav} platform. Although 3D sonar sensors, such as TOPOSENS 3D Ultrasonic Sensors introduced in \cite{Toposens2020}, provide image-like distance measurements, they are also heavy and power-consuming, making integration onto a \gls{uav} not feasible.

The polarization camera is an intriguing alternative sensor worth considering. When light reflects off a glass surface, it becomes partially polarized, meaning that the reflected light waves oscillate in a particular direction. A polarization camera is able to detect this polarization by measuring the orientation of the oscillation of the reflected light waves. This allows the camera to distinguish between glass surfaces and other materials that do not polarize light in the same way, such as walls or desks. There exists several work which use polarization cues for transparent object segmentation \cite{deeppolar} and glass segmentation \cite{glasspolar}. However, this field still holds great potential for further investigation. One possible direction for future research is to integrate the polarization camera onto the \gls{uav} platform and fuse the data provided by the polarization camera and RGB-D camera using learning-based approach. Doing so may create a more reliable mechanism for window segmentation, thus improving the safety of \gls{uav} navigation and providing valuable semantic information for exploration tasks.
