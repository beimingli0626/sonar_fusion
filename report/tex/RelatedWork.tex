\section{Related Work}
\label{sec:related work}

% Exploration Strategy: Frontier based, Information based (sampling based), Mixed(hierarchical) approach
% Map Prediction: database, deep learning

\subsection{Ultrasonic Sensor Model}
The literature contains various models for ultrasonic sensors that are used in robot mapping and navigation applications. For example, in \cite{mapping2017} and \cite{advanced_sonar_sensor}, sonars are modeled as a 2D circular sector, where the sector angle and radius are determined by the sensing range of the hardware. On the other hand, \cite{multiray} proposes a model where the sonar beam is discretized into multiple rays, originating from the sensor center. This approach allows for a reduction in computational complexity and a more accurate representation of the irregular polygon shape of the sonar beam.

Given the beam pattern of the actual sensor attached to our UAV platform, we determined that it was unnecessary to adapt the multiple-ray discretization approach. However, since we also required the ability to perform 3D mapping, we modeled the sonar beam as a spherical cone. This approach extends the 2D circular sector model to its 3D counterpart and provides us with the necessary data to update and maintain a 3D occupancy map using sonar readings.

To accomplish this, we implemented an \gls{ism}, which encodes the probability that a given location in the environment contains an object. We extended the 2D \gls{ism} proposed in \cite{advanced_sonar_sensor} and \cite{wideanglesonar} to 3D to update and maintain a 3D occupancy map with sonar readings. Further details on our approach will be discussed in section \ref{sec:method_model}.

\subsection{Sensor Fusion with Ultrasonic Sensor}
Multiple studies have been conducted to integrate the ultrasonic sensor with either a camera or a LiDAR for robot mapping and navigation purposes. One such work is presented in \cite{lidarfusion}, where a fusion approach combining the ultrasonic sensor with a 2D LiDAR is proposed. Their method involves maintaining a single occupancy map, wherein the probability of each grid being occupied is updated at each time step by fusing sensor outputs. Notably, various sensor fusion algorithms, including the Linear Opinion Pool and the classic Bayes Filter, are evaluated to determine their effectiveness.

In addition to LiDAR, combining ultrasonic sensors with cameras has been investigated by several studies such as \cite{camerafusion} and \cite{obsavoid}. Of these, \cite{camerafusion} is particularly relevant to our task, as it has demonstrated the ability to detect moderate-sized glass walls. Unlike \cite{lidarfusion}, \cite{camerafusion} maintains two separate maps for depth camera and sonar data. The sonar map is updated only when the sonar reading provides information not present in the camera-based map. Finally, the two maps are merged into a single occupancy map. 

\subsection{Glass Segmentation}
The works discussed above have successfully addressed the issue of detecting glass as obstacles to enhance the safety of robot navigation. However, none of these works can provide segmentation information, i.e., the ability to distinguish glass from other semantic objects. Many existing studies have tackled the problem of glass segmentation using learning-based methods that take a single RGB or RGB-D image as input and generate a semantic mask for the glass region as output, as discussed in \cite{depthaware} and \cite{donthit}. However, such methods may prove to be computationally burdensome for UAV platforms, which are already limited in computational power and may not be able to support another large network solely for glass segmentation. Instead of learning-based methods, we propose a RANSAC-based segmentation approach. In indoor scenarios, most glass is typically windows framed by walls. Given a depth-camera-based occupancy map, RANSAC could detect the largest plane, which should correspond to straight walls in an indoor setting. We could then locate potential window regions based on geometric information. Further details on this approach will be provided in Section \ref{sec:method_ransac}.